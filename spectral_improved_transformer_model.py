# -*- coding: utf-8 -*-
"""spectral_improved_transformer_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Uz-AK8nD7Bmaku_tgGj-8z8RiRsNIzLi
"""

import os
from typing import Dict, List, Tuple, Callable, Optional
from dataclasses import dataclass, field
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, random_split
import matplotlib.pyplot as plt

@dataclass
class ModelConfig:
    """Configuration for the transformer model."""
    seq_len: int = 720
    d_model: int = 64
    nhead: int = 4
    num_layers: int = 6
    dim_feedforward: int = 256
    dropout: float = 0.1
    n_features: int = 3  # Bx, By, Bz

@dataclass
class TrainingConfig:
    batch_size: int = 32
    val_split: float = 0.2
    num_epochs: int = 2
    learning_rate: float = 5e-5
    weight_decay: float = 1e-3
    checkpoint_dir: str = "./checkpoints"
    save_every: int = 10
    alpha_schedule: Callable[[int], float] = field(default_factory=lambda: (lambda e: 0.3+ 0.7*e/24))
    spectral_weight: float = 1.0
    intermittency_weight: float = 1.0

@dataclass
class LossComponents:
    """Container for tracking individual loss components."""
    total: float = 0.0
    mse: float = 0.0
    spectral: float = 0.0
    intermittency: float = 0.0
    physics: float = 0.0

    def __add__(self, other):
        return LossComponents(
            total=self.total + other.total,
            mse=self.mse + other.mse,
            spectral=self.spectral + other.spectral,
            intermittency=self.intermittency + other.intermittency,
            physics=self.physics + other.physics
        )

    def __truediv__(self, value):
        return LossComponents(
            total=self.total / value,
            mse=self.mse / value,
            spectral=self.spectral / value,
            intermittency=self.intermittency / value,
            physics=self.physics / value
        )

import torch
import torch.nn.functional as F
from typing import List, Tuple


def spectral_loss(history: torch.Tensor,
                  pred: torch.Tensor,
                  tgt: torch.Tensor,
                  n_freqs: int = 50,
                  eps: float = 1e-6) -> torch.Tensor:
    """
    FFT-based spectral loss: how PSD magnitude and phase change when appending pred vs. true tgt.

    Args:
        history: [B, T, 6]  past sequence with [Bx, By, Bz, ΔBx, ΔBy, ΔBz]
        pred:    [B, 3]     predicted next point
        tgt:     [B, 3]     ground-truth next point
        n_freqs: number of frequency bins to sample (up to Nyquist)
        eps:     small offset for log stability
    Returns:
        scalar loss = log-magnitude MSE + phase alignment penalty
    """
    B, T, _ = history.shape
    device = history.device

    # Extract only the value components (first 3 features)
    history_values = history[:, :, :3]  # [B, T, 3]

    # Construct sequences using only values
    seq_pred = torch.cat([history_values, pred.unsqueeze(1)], dim=1)  # [B, T+1, 3]
    seq_true = torch.cat([history_values, tgt.unsqueeze(1)], dim=1)

    # Compute real FFT
    fft_pred = torch.fft.rfft(seq_pred, dim=1)  # [B, F, 3]
    fft_true = torch.fft.rfft(seq_true, dim=1)

    Fmax = fft_pred.shape[1]
    # Select frequency indices 1..Fmax-1 evenly
    idx = torch.linspace(1, Fmax - 1, min(n_freqs, Fmax - 1), device=device)
    idx = idx.round().long().unique()

    # Sample FFT
    fp = fft_pred[:, idx, :]  # [B, n_freqs, 3]
    ft = fft_true[:, idx, :]

    # Magnitude and phase
    mag_p = fp.abs()
    mag_t = ft.abs()
    phase_p = torch.angle(fp)
    phase_t = torch.angle(ft)

    # Log-magnitude MSE
    mag_loss = F.mse_loss((mag_p + eps).log(), (mag_t + eps).log())

    # Wrapped phase difference
    dphi = phase_p - phase_t
    dphi = torch.atan2(torch.sin(dphi), torch.cos(dphi))
    phase_loss = (1 - torch.cos(dphi)).mean()

    return mag_loss + phase_loss


def intermittency_loss(history: torch.Tensor,
                       next_pt: torch.Tensor,
                       lags: List[int] = None,
                       orders: List[int] = None,
                       alpha: float = 1.0) -> torch.Tensor:
    """Change in multi-scale structure functions.

    Args:
        history: [B, T, 6] with [Bx, By, Bz, ΔBx, ΔBy, ΔBz]
        next_pt: [B, 3] predicted next values
    """
    if lags is None:
        lags = [1, 2, 4, 8, 16, 32, 64]
    if orders is None:
        orders = [1, 2, 4, 6, 8]

    # Extract only the value components (first 3 features) for structure functions
    history_values = history[:, :, :3]  # [B, T, 3]

    # Build old and new sequences
    seq_old = history_values
    seq_new = torch.cat([history_values, next_pt.unsqueeze(1)], dim=1)
    B, T, C = seq_old.shape

    # Weights for each (lag, order)
    ws = torch.tensor([o * (l ** -alpha) for l in lags for o in orders]).to(history.device)
    ws = ws / ws.sum()
    eps = 1e-12
    loss = 0.0
    idx = 0

    for l in lags:
        for o in orders:
            inc_old = torch.norm(seq_old[:, l:] - seq_old[:, :-l], dim=2).pow(o)
            inc_new = torch.norm(seq_new[:, l:] - seq_new[:, :-l], dim=2).pow(o)

            S_old = inc_old.mean(1) + eps
            S_new = inc_new.mean(1) + eps
            loss += ws[idx] * (S_new.log() - S_old.log()).abs().mean()
            idx += 1

    return loss * 40


def physics_informed_loss(history: torch.Tensor,
                          pred: torch.Tensor,
                          tgt: torch.Tensor,
                          spec_w: float = 1.0,
                          inter_w: float = 1.0) -> torch.Tensor:
    """Weighted sum of spectral and intermittency losses.

    Args:
        history: [B, T, 6] with [Bx, By, Bz, ΔBx, ΔBy, ΔBz]
        pred: [B, 3] predicted values
        tgt: [B, 3] target values
    """
    spec = spectral_loss(history, pred, tgt)
    inter = intermittency_loss(history, pred)
    return spec_w * spec + inter_w * inter


def composite_loss_detailed(pred: torch.Tensor,
                           tgt: torch.Tensor,
                           history: torch.Tensor,
                           alpha: float,
                           spec_w: float = 1.0,
                           inter_w: float = 1.0) -> Tuple[torch.Tensor, LossComponents]:
    """Combined MSE and physics-informed loss with detailed component tracking."""
    # MSE component
    mse_loss = F.l1_loss(pred, tgt) / 10

    # Physics-informed components
    spec_loss = spectral_loss(history, pred, tgt)
    inter_loss = intermittency_loss(history, pred)
    phys_loss = spec_w * spec_loss + inter_w * inter_loss

    # Total
    total_loss = (1 - alpha) * mse_loss + alpha * phys_loss

    components = LossComponents(
        total=total_loss.item(),
        mse=mse_loss.item(),
        spectral=spec_loss.item(),
        intermittency=inter_loss.item(),
        physics=phys_loss.item()
    )

    return total_loss, components


class TurbulenceDataset(Dataset):
    """Dataset for 3D turbulence time series data with global normalization."""

    def __init__(self, npz_path: str, seq_len: int = 720, max_days: Optional[int] = None,
                 max_b_threshold: float = 40.0, normalize: bool = True):
        """
        Initialize dataset from NPZ file with global normalization.

        Args:
            npz_path: Path to NPZ file containing time series data
            seq_len: Length of input sequences
            max_days: Maximum number of days to load (for debugging)
            max_b_threshold: Maximum allowed |B| value for physical filtering
            normalize: Whether to apply global normalization
        """
        self.seq_len = seq_len
        self.max_b_threshold = max_b_threshold
        self.normalize = normalize

        # Load and filter data
        self.days, self.filtered_stats = self._load_data(npz_path, max_days)

        # Compute global statistics BEFORE creating windows
        if self.normalize:
            self.global_mean, self.global_std = self._compute_global_stats()
        else:
            self.global_mean = np.zeros(3, dtype=np.float32)
            self.global_std = np.ones(3, dtype=np.float32)

        # Pre-calculate windows and indices for efficiency
        self.day_lengths = [len(day) for day in self.days]
        self.windows_per_day = self._calculate_windows_per_day()
        self.total_windows = sum(self.windows_per_day)

        # Pre-compute cumulative windows for faster indexing
        self.cumulative_windows = np.cumsum([0] + self.windows_per_day)

        print(f"Dataset Statistics:")
        print(f"  Loaded {len(self.days)} days (after filtering)")
        print(f"  Filtered out {self.filtered_stats['non_physical_days']} non-physical days")
        print(f"  Filtered out {self.filtered_stats['short_days']} days too short")
        print(f"  Total training examples: {self.total_windows}")
        print(f"  Average day length: {np.mean(self.day_lengths):.1f} points")
        if self.normalize:
            print(f"  Global mean (Bx, By, Bz): {self.global_mean}")
            print(f"  Global std (Bx, By, Bz): {self.global_std}")

    def _load_data(self, npz_path: str, max_days: Optional[int]) -> Tuple[List[np.ndarray], Dict]:
        """Load and validate 3D time series data from NPZ file."""
        if not os.path.exists(npz_path):
            raise FileNotFoundError(f"NPZ file not found: {npz_path}")

        data = np.load(npz_path, allow_pickle=True)
        days = []

        stats = {
            'total_days': 0,
            'non_physical_days': 0,
            'short_days': 0,
            'valid_days': 0
        }

        # Handle different data formats
        if 'vectors' in data:
            raw_vectors = data['vectors']
        else:
            # Assume data keys are day indices
            raw_vectors = [data[key] for key in sorted(data.files)]

        total_days_available = len(raw_vectors)
        if max_days is not None:
            total_days_available = min(total_days_available, max_days)

        for i in range(total_days_available):
            stats['total_days'] += 1
            day = raw_vectors[i]

            # Convert to numpy array if needed
            if not isinstance(day, np.ndarray):
                try:
                    day = np.array(day, dtype=np.float32)
                except:
                    stats['non_physical_days'] += 1
                    continue
            else:
                day = day.astype(np.float32)

            # Validate shape
            if day.ndim != 2 or day.shape[1] != 3:
                stats['non_physical_days'] += 1
                continue

            # Remove NaN and inf values
            valid_mask = np.isfinite(day).all(axis=1)
            day = day[valid_mask]

            if len(day) == 0:
                stats['non_physical_days'] += 1
                continue

            # Check magnetic field magnitude for physical validity
            b_magnitude = np.linalg.norm(day, axis=1)
            physical_points = np.sum(b_magnitude <= self.max_b_threshold)
            physical_percentage = physical_points / len(day)

            # Require at least 80% of points to be within physical range
            if physical_percentage < 0.8:
                stats['non_physical_days'] += 1
                continue

            # Apply filtering to remove extreme outliers
            physical_mask = b_magnitude <= self.max_b_threshold
            day = day[physical_mask]

            # Check if day has sufficient length after filtering
            if len(day) < self.seq_len + 1:
                stats['short_days'] += 1
                continue

            days.append(day)
            stats['valid_days'] += 1

        if not days:
            raise ValueError(f"No valid time series found in {npz_path} after filtering")
        return days, stats

    def _compute_global_stats(self) -> Tuple[np.ndarray, np.ndarray]:
        """Compute global mean and std across all valid data points."""
        # Concatenate all days
        all_data = np.concatenate(self.days, axis=0)  # Shape: [total_points, 3]

        # Compute per-component statistics
        global_mean = np.mean(all_data, axis=0).astype(np.float32)  # Shape: [3]
        global_std = np.std(all_data, axis=0).astype(np.float32)    # Shape: [3]

        # Prevent division by zero
        global_std = np.maximum(global_std, 1e-6)

        return global_mean, global_std

    def _calculate_windows_per_day(self) -> List[int]:
        """Calculate number of valid windows per day."""
        return [max(0, length - self.seq_len) for length in self.day_lengths]

    def __len__(self) -> int:
        """Return total number of windows."""
        return self.total_windows

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Get a single sample with global normalization applied, then compute increments.

        Returns:
            history: [seq_len, 6] tensor with [Bx, By, Bz, ΔBx, ΔBy, ΔBz]
            target: [3] tensor with normalized [Bx, By, Bz]
        """
        if idx < 0 or idx >= self.total_windows:
            raise IndexError(f"Index {idx} out of range [0, {self.total_windows})")

        # Find which day this index belongs to (binary search for efficiency)
        day_idx = np.searchsorted(self.cumulative_windows, idx, side='right') - 1
        window_idx = idx - self.cumulative_windows[day_idx]

        day_data = self.days[day_idx]

        # Extract raw data
        raw_hist = day_data[window_idx:window_idx + self.seq_len]  # [seq_len, 3]
        raw_tgt = day_data[window_idx + self.seq_len]  # [3]

        # Apply global normalization if enabled
        if self.normalize:
            hist_normalized = (raw_hist - self.global_mean) / self.global_std
            tgt_normalized = (raw_tgt - self.global_mean) / self.global_std
        else:
            hist_normalized = raw_hist
            tgt_normalized = raw_tgt

        # Convert to torch tensors
        values = torch.tensor(hist_normalized, dtype=torch.float32)  # [seq_len, 3]
        target = torch.tensor(tgt_normalized, dtype=torch.float32)    # [3]

        # Compute increments from normalized values
        increments = torch.zeros_like(values)  # [seq_len, 3]
        increments[1:] = values[1:] - values[:-1]
        # First increment is zero (no previous point)
        increments[0] = 0

        # Concatenate values and increments to get 6 features
        history = torch.cat([values, increments], dim=1)  # [seq_len, 6]

        return history, target

    def denormalize(self, data: torch.Tensor) -> torch.Tensor:
        """
        Denormalize data back to original scale.

        Args:
            data: Normalized tensor of shape [..., 3]

        Returns:
            Denormalized tensor in original scale
        """
        if not self.normalize:
            return data

        device = data.device
        mean = torch.tensor(self.global_mean, device=device, dtype=data.dtype)
        std = torch.tensor(self.global_std, device=device, dtype=data.dtype)

        return data * std + mean

    def get_normalization_params(self) -> Tuple[np.ndarray, np.ndarray]:
        """Return global normalization parameters."""
        return self.global_mean.copy(), self.global_std.copy()


class TurbulenceTransformer(nn.Module):
    """Transformer model for 3D turbulence next-point prediction with 6 input features."""

    def __init__(self, config: ModelConfig):
        super().__init__()
        self.config = config

        # Input embeddings for 6D data (Bx, By, Bz, ΔBx, ΔBy, ΔBz)
        self.value_embedding = nn.Linear(6, config.d_model)  # Changed from 3 to 6
        self.pos_embedding = nn.Embedding(config.seq_len, config.d_model)

        # Transformer encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=config.d_model,
            nhead=config.nhead,
            dim_feedforward=config.dim_feedforward,
            dropout=config.dropout,
            activation='gelu',
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, config.num_layers)

        # Output layer for 3D prediction (still predicting Bx, By, Bz)
        self.output_layer = nn.Linear(config.d_model, 3)  # Still output 3 values

        # Initialize weights
        self._init_weights()

    def _init_weights(self):
        """Initialize model weights."""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
                if module.bias is not None:
                    torch.nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Embedding):
                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass.

        Args:
            x: Input sequences [B, T, 6] with [Bx, By, Bz, ΔBx, ΔBy, ΔBz]

        Returns:
            Next-point predictions [B, 3] for [Bx, By, Bz]
        """
        if x.dim() != 3 or x.size(-1) != 6:
            raise ValueError(f"Expected input shape [B, T, 6], got {x.shape}")

        B, T, _ = x.shape

        # Value embeddings
        x_embedded = self.value_embedding(x)  # [B, T, d_model]

        # Position embeddings
        positions = torch.arange(T, device=x.device).unsqueeze(0).expand(B, -1)
        pos_embedded = self.pos_embedding(positions)  # [B, T, d_model]

        # Combine embeddings
        hidden = x_embedded + pos_embedded

        # Transformer encoding
        encoded = self.transformer(hidden)  # [B, T, d_model]

        # Output projection (use last timestep)
        output = self.output_layer(encoded[:, -1, :])  # [B, 3]

        return output

# Checkpoint functions remain the same
def save_checkpoint(model: nn.Module, optimizer: torch.optim.Optimizer,
                   epoch: int, loss: float, checkpoint_dir: str,
                   train_losses: List[LossComponents], val_losses: List[LossComponents],
                   is_best: bool = False) -> None:
    """Save model checkpoint with loss history."""
    os.makedirs(checkpoint_dir, exist_ok=True)

    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
        'train_losses': train_losses,
        'val_losses': val_losses,
    }

    checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}.pth')
    torch.save(checkpoint, checkpoint_path)

    if is_best:
        best_path = os.path.join(checkpoint_dir, 'best_model.pth')
        torch.save(checkpoint, best_path)
        print(f"Best model saved at epoch {epoch}")

    latest_path = os.path.join(checkpoint_dir, 'latest_checkpoint.pth')
    torch.save(checkpoint, latest_path)

    print(f"Checkpoint saved: {checkpoint_path}")

def load_checkpoint(model: nn.Module, optimizer: torch.optim.Optimizer,
                   checkpoint_path: str) -> Tuple[int, List[LossComponents], List[LossComponents]]:
    """Load model checkpoint with loss history."""
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")

    checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

    epoch = checkpoint['epoch']
    train_losses = checkpoint.get('train_losses', [])
    val_losses = checkpoint.get('val_losses', [])

    print(f"Loaded checkpoint from epoch {epoch}")
    return epoch, train_losses, val_losses

def load_latest_checkpoint(model: nn.Module, optimizer: torch.optim.Optimizer,
                          checkpoint_dir: str) -> Tuple[int, List[LossComponents], List[LossComponents]]:
    """Load the latest checkpoint if available."""
    latest_path = os.path.join(checkpoint_dir, 'latest_checkpoint.pth')
    if os.path.exists(latest_path):
        return load_checkpoint(model, optimizer, latest_path)
    return 0, [], []

def save_detailed_training_plots(train_losses: List[LossComponents], val_losses: List[LossComponents],
                               checkpoint_dir: str) -> None:
    """Save comprehensive training and validation loss plots."""
    if not train_losses or not val_losses:
        print("No loss data to plot")
        return

    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('3D Training Progress - Loss Components Over Epochs', fontsize=16)

    epochs = range(len(train_losses))

    # Extract loss components
    train_total = [loss.total for loss in train_losses]
    train_mse = [loss.mse for loss in train_losses]
    train_spectral = [loss.spectral for loss in train_losses]
    train_intermittency = [loss.intermittency for loss in train_losses]
    train_physics = [loss.physics for loss in train_losses]

    val_total = [loss.total for loss in val_losses]
    val_mse = [loss.mse for loss in val_losses]
    val_spectral = [loss.spectral for loss in val_losses]
    val_intermittency = [loss.intermittency for loss in val_losses]
    val_physics = [loss.physics for loss in val_losses]

    # Plot configurations
    plot_configs = [
        (axes[0, 0], train_total, val_total, 'Total Loss'),
        (axes[0, 1], train_mse, val_mse, 'MSE Loss'),
        (axes[0, 2], train_physics, val_physics, 'Physics Loss'),
        (axes[1, 0], train_spectral, val_spectral, 'Spectral Loss'),
        (axes[1, 1], train_intermittency, val_intermittency, 'Intermittency Loss')
    ]

    for ax, train_data, val_data, title in plot_configs:
        ax.plot(epochs, train_data, 'b-', label='Training', linewidth=2)
        ax.plot(epochs, val_data, 'r-', label='Validation', linewidth=2)
        ax.set_title(title)
        ax.set_xlabel('Epoch')
        ax.set_ylabel('Loss')
        ax.legend()
        ax.grid(True, alpha=0.3)

    # Log-scale total loss
    axes[1, 2].semilogy(epochs, train_total, 'b-', label='Training', linewidth=2)
    axes[1, 2].semilogy(epochs, val_total, 'r-', label='Validation', linewidth=2)
    axes[1, 2].set_title('Total Loss (Log Scale)')
    axes[1, 2].set_xlabel('Epoch')
    axes[1, 2].set_ylabel('Loss (log)')
    axes[1, 2].legend()
    axes[1, 2].grid(True, alpha=0.3)

    plt.tight_layout()
    plot_path = os.path.join(checkpoint_dir, 'detailed_training_losses_3d.png')
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"3D training plots saved to {plot_path}")

def train_epoch_with_detailed_tracking(model, train_loader, optimizer, device, epoch, config):
    """Train for one epoch with detailed loss component tracking."""
    n_batches = len(train_loader)
    model.train()

    total_loss_components = LossComponents()
    valid_batches = 0
    skipped_batches = 0
    alpha = config.alpha_schedule(epoch)

    print(f"[Epoch {epoch}] Alpha = {alpha:.3f}")

    for batch_idx, (history, targets) in enumerate(train_loader):
        history, targets = history.to(device), targets.to(device)
        predictions = model(history)

        loss, loss_components = composite_loss_detailed(
            predictions, targets, history, alpha,
            config.spectral_weight, config.intermittency_weight
        )

        if not torch.isfinite(loss):
            print(f"Skipping non-finite loss at epoch {epoch}, batch {batch_idx}")
            skipped_batches += 1
            continue

        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()

        total_loss_components = total_loss_components + loss_components
        valid_batches += 1

        if (batch_idx + 1) % max(1, n_batches // 10) == 0:
            progress = (batch_idx + 1) / n_batches * 100
            print(f"  Progress: {progress:.0f}% - Batch Loss: {loss.item():.4f}")

    if valid_batches == 0:
        return LossComponents(total=float('inf'))

    avg_loss_components = total_loss_components / valid_batches

    if skipped_batches > 0:
        print(f"Epoch {epoch}: skipped {skipped_batches} batches due to non-finite loss.")

    print(f"[Epoch {epoch}] Train - Total: {avg_loss_components.total:.4f}, "
          f"MSE: {avg_loss_components.mse:.4f}, Physics: {avg_loss_components.physics:.4f}")

    return avg_loss_components

def validate_model_detailed(model: nn.Module, val_loader: DataLoader, device: torch.device,
                          epoch: int, config: TrainingConfig) -> LossComponents:
    """Validate the model with detailed loss component tracking."""
    model.eval()
    total_loss_components = LossComponents()
    valid_batches = 0
    alpha = config.alpha_schedule(epoch)

    with torch.no_grad():
        for history, targets in val_loader:
            history = history.to(device)
            targets = targets.to(device)

            predictions = model(history)
            loss, loss_components = composite_loss_detailed(
                predictions, targets, history, alpha,
                config.spectral_weight, config.intermittency_weight
            )

            if torch.isfinite(loss):
                total_loss_components = total_loss_components + loss_components
                valid_batches += 1
            else:
                print(f"[Epoch {epoch}] Skipping non-finite val loss")

    if valid_batches == 0:
        return LossComponents(total=float('inf'))

    avg_loss_components = total_loss_components / valid_batches

    print(f"[Epoch {epoch}] Val   - Total: {avg_loss_components.total:.4f}, "
          f"MSE: {avg_loss_components.mse:.4f}, Physics: {avg_loss_components.physics:.4f}")

    return avg_loss_components

def train_model(dataset: TurbulenceDataset, model_config: ModelConfig,
               training_config: TrainingConfig) -> None:
    """Main training function with enhanced loss tracking for 3D data."""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    # Split dataset
    val_size = int(len(dataset) * training_config.val_split)
    train_size = len(dataset) - val_size
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

    print(f"Dataset split: {train_size} training, {val_size} validation samples")

    # Create data loaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=training_config.batch_size,
        shuffle=True,
        num_workers=2
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=training_config.batch_size,
        num_workers=2
    )

    # Initialize model and optimizer
    model = TurbulenceTransformer(model_config).to(device)
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=training_config.learning_rate,
        weight_decay=training_config.weight_decay
    )

    print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")

    # Try to load existing checkpoint
    start_epoch, train_losses, val_losses = load_latest_checkpoint(
        model, optimizer, training_config.checkpoint_dir
    )
    print(f"Starting training from epoch {start_epoch}")

    # Training loop
    best_val_loss = float('inf')
    if val_losses:
        best_val_loss = min(loss.total for loss in val_losses)

    for epoch in range(start_epoch, training_config.num_epochs):
        print(f"\n{'='*50}")
        print(f"Epoch {epoch}/{training_config.num_epochs-1}")
        print(f"{'='*50}")

        # Training
        train_loss = train_epoch_with_detailed_tracking(
            model, train_loader, optimizer, device, epoch, training_config
        )
        train_losses.append(train_loss)

        # Validation
        val_loss = validate_model_detailed(model, val_loader, device, epoch, training_config)
        val_losses.append(val_loss)

        # Check if this is the best model
        is_best = val_loss.total < best_val_loss
        if is_best:
            best_val_loss = val_loss.total

        # Save checkpoint
        if epoch % training_config.save_every == 0 or is_best or epoch == training_config.num_epochs - 1:
            save_checkpoint(model, optimizer, epoch, val_loss.total,
                          training_config.checkpoint_dir, train_losses, val_losses, is_best)

        # Save plots periodically
        if epoch % 5 == 0 or epoch == training_config.num_epochs - 1:
            save_detailed_training_plots(train_losses, val_losses, training_config.checkpoint_dir)

    print(f"\nTraining completed!")
    print(f"Best validation loss: {best_val_loss:.4f}")

    # Final comprehensive plots
    save_detailed_training_plots(train_losses, val_losses, training_config.checkpoint_dir)


# Configuration for 3D model
model_config = ModelConfig(
    seq_len=200,
    d_model=64,
    nhead=8,
    num_layers=6,
    dim_feedforward=256,
    dropout=0.1,
    n_features=3  # Bx, By, Bz
)

training_config = TrainingConfig(
    batch_size=32,
    val_split=0.2,
    num_epochs=24,
    learning_rate=5e-4,
    weight_decay=1e-3,
    checkpoint_dir="./checkpoints_3d",
    save_every=1
)

# Dataset with 3D filtering
dataset = TurbulenceDataset(
    npz_path="./daily_B_vectors.npz",
    seq_len=model_config.seq_len,
    max_days=None,  # Use all available days
    max_b_threshold=50.0  # Filter out non-physical days based on |B| magnitude
)

# Train the model
train_model(dataset, model_config, training_config)