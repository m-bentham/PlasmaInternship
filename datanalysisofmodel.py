# -*- coding: utf-8 -*-
"""DatAnalysisofModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X3Ff6YGaSgABbxuBuGabVABu5THwskY7

This code takes the model produced in the model training file and analyses the results.
"""

import os
from typing import Dict, List, Tuple, Callable, Optional
from dataclasses import dataclass, field
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, random_split
import matplotlib.pyplot as plt

@dataclass
class ModelConfig:
    """Configuration for the transformer model."""
    seq_len: int = 720
    d_model: int = 64
    nhead: int = 4
    num_layers: int = 6
    dim_feedforward: int = 256
    dropout: float = 0.1
    n_features: int = 3  # Bx, By, Bz

@dataclass
class LossComponents:
    total: float = 0.0
    mse: float = 0.0
    spectral: float = 0.0
    intermittency: float = 0.0
    physics: float = 0.0

@dataclass
class TrainingConfig:
    batch_size: int = 32
    val_split: float = 0.2
    num_epochs: int = 2
    learning_rate: float = 5e-5
    weight_decay: float = 1e-3
    checkpoint_dir: str = "./checkpoints"
    save_every: int = 10
    alpha_schedule: Callable[[int], float] = field(default_factory=lambda: (lambda e: 0.3+ 0.7*e/24))
    spectral_weight: float = 1.0
    intermittency_weight: float = 1.0

class TurbulenceDataset(Dataset):
    """Dataset for 3D turbulence time series data (Bx, By, Bz)."""

    def __init__(self, npz_path: str, seq_len: int = 720, max_days: Optional[int] = None,
                 max_b_threshold: float = 40.0):
        """
        Initialize dataset from NPZ file.

        Args:
            npz_path: Path to NPZ file containing time series data
            seq_len: Length of input sequences
            max_days: Maximum number of days to load (for debugging)
            max_b_threshold: Maximum allowed |B| value for physical filtering
        """
        self.seq_len = seq_len
        self.max_b_threshold = max_b_threshold
        self.days, self.filtered_stats = self._load_data(npz_path, max_days)
        self.day_lengths = [len(day) for day in self.days]
        self.windows_per_day = self._calculate_windows_per_day()
        self.total_windows = sum(self.windows_per_day)

        print(f"Dataset Statistics:")
        print(f"  Loaded {len(self.days)} days (after filtering)")
        print(f"  Filtered out {self.filtered_stats['non_physical_days']} non-physical days (|B| > {max_b_threshold})")
        print(f"  Filtered out {self.filtered_stats['short_days']} days too short (< {seq_len + 1} points)")
        print(f"  Total training examples: {self.total_windows}")
        print(f"  Average day length: {np.mean(self.day_lengths):.1f} points")

    def _load_data(self, npz_path: str, max_days: Optional[int]) -> Tuple[List[np.ndarray], Dict]:
        """Load and validate 3D time series data from NPZ file."""
        if not os.path.exists(npz_path):
            raise FileNotFoundError(f"NPZ file not found: {npz_path}")

        data = np.load(npz_path, allow_pickle=True)
        days = []

        stats = {
            'total_days': 0,
            'non_physical_days': 0,
            'short_days': 0,
            'valid_days': 0
        }

        raw_vectors = data['vectors']
        total_days_available = len(raw_vectors)

        if max_days is not None:
            total_days_available = min(total_days_available, max_days)

        for i in range(total_days_available):
            stats['total_days'] += 1
            day = raw_vectors[i]  # Expected shape: [N, 3] for Bx, By, Bz

            if day.shape[1] != 3:
                print(f"Warning: Day {i} has shape {day.shape}, expected [N, 3]. Skipping.")
                stats['non_physical_days'] += 1
                continue

            # Remove NaN and inf values
            valid_mask = np.isfinite(day).all(axis=1)
            day = day[valid_mask]

            if len(day) == 0:
                stats['non_physical_days'] += 1
                continue

            # Check magnetic field magnitude for physical validity
            b_magnitude = np.linalg.norm(day, axis=1)
            physical_points = np.sum(b_magnitude <= self.max_b_threshold)
            physical_percentage = physical_points / len(day)

            # Require at least 80% of points to be within physical range
            if physical_percentage < 0.8:
                stats['non_physical_days'] += 1
                print(f"Day {i}: Filtered out - only {physical_percentage:.1%} points within |B| <= {self.max_b_threshold}")
                continue

            # Apply filtering to remove extreme outliers
            physical_mask = b_magnitude <= self.max_b_threshold
            day = day[physical_mask]

            # Check if day has sufficient length after filtering
            if len(day) < self.seq_len + 1:
                stats['short_days'] += 1
                continue

            days.append(day)
            stats['valid_days'] += 1

        if not days:
            raise ValueError(f"No valid time series found in {npz_path} after filtering")

        return days, stats

    def _calculate_windows_per_day(self) -> List[int]:
        return [max(0, length - self.seq_len) for length in self.day_lengths]

    def __len__(self) -> int:
        return sum(self.windows_per_day)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        day_idx = 0
        while idx >= self.windows_per_day[day_idx]:
            idx -= self.windows_per_day[day_idx]
            day_idx += 1
        window_idx = idx
        day_data = self.days[day_idx]

        # Extract raw numpy arrays - 3D data
        raw_hist = day_data[window_idx:window_idx + self.seq_len]  # [seq_len, 3]
        raw_tgt = day_data[window_idx + self.seq_len]  # [3]

        # Convert to torch tensors
        history = torch.tensor(raw_hist, dtype=torch.float32)  # [seq_len, 3]
        target = torch.tensor(raw_tgt, dtype=torch.float32)    # [3]

        # Z-score normalize each component separately
        eps = 1e-6
        for c in range(3):  # For each component (Bx, By, Bz)
            component_data = history[:, c]
            mean = component_data.mean()
            std = component_data.std()
            history[:, c] = (component_data - mean) / (std + eps)
            target[c] = (target[c] - mean) / (std + eps)

        return history, target

class TurbulenceTransformer(nn.Module):
    """Transformer model for 3D turbulence next-point prediction."""

    def __init__(self, config: ModelConfig):
        super().__init__()
        self.config = config

        # Input embeddings for 3D data
        self.value_embedding = nn.Linear(config.n_features, config.d_model)
        self.pos_embedding = nn.Embedding(config.seq_len, config.d_model)

        # Transformer encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=config.d_model,
            nhead=config.nhead,
            dim_feedforward=config.dim_feedforward,
            dropout=config.dropout,
            activation='gelu',
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, config.num_layers)

        # Output layer for 3D prediction
        self.output_layer = nn.Linear(config.d_model, config.n_features)

        # Initialize weights
        self._init_weights()

    def _init_weights(self):
        """Initialize model weights."""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
                if module.bias is not None:
                    torch.nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Embedding):
                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass.

        Args:
            x: Input sequences [B, T, 3]

        Returns:
            Next-point predictions [B, 3]
        """
        if x.dim() != 3 or x.size(-1) != self.config.n_features:
            raise ValueError(f"Expected input shape [B, T, {self.config.n_features}], got {x.shape}")

        B, T, _ = x.shape

        # Value embeddings
        x_embedded = self.value_embedding(x)  # [B, T, d_model]

        # Position embeddings
        positions = torch.arange(T, device=x.device).unsqueeze(0).expand(B, -1)
        pos_embedded = self.pos_embedding(positions)  # [B, T, d_model]

        # Combine embeddings
        hidden = x_embedded + pos_embedded

        # Transformer encoding
        encoded = self.transformer(hidden)  # [B, T, d_model]

        # Output projection (use last timestep)
        output = self.output_layer(encoded[:, -1, :])  # [B, 3]

        return output

import torch
import numpy as np
import matplotlib.pyplot as plt
import os
from scipy import signal, stats
from scipy.stats import gennorm
import warnings
warnings.filterwarnings('ignore')

# Minimal class definition to handle checkpoint loading
from dataclasses import dataclass

@dataclass
class LossComponents:
    total: float = 0.0
    mse: float = 0.0
    spectral: float = 0.0
    intermittency: float = 0.0
    physics: float = 0.0

def load_model(config, checkpoint_dir):
    """Load model from checkpoint."""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = TurbulenceTransformer(config).to(device)

    checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint_epoch_23.pth')
    if not os.path.exists(checkpoint_path):
        checkpoint_path = os.path.join(checkpoint_dir, 'latest_checkpoint.pth')

    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    return model

def get_data(dataset, seed_len, gen_len, day_idx=None):
    """Get normalized seed and ground truth."""
    if day_idx is None:
        day_idx = next(i for i, l in enumerate(dataset.day_lengths) if l >= seed_len + gen_len)

    day = dataset.days[day_idx]
    start = np.random.randint(0, len(day) - seed_len - gen_len + 1)

    raw_seed = day[start:start + seed_len]
    raw_full = day[start:start + seed_len + gen_len]

    # Normalize using seed stats only
    mean, std = raw_seed.mean(axis=0), raw_seed.std(axis=0) + 1e-6
    norm_seed = (raw_seed - mean) / std
    norm_full = (raw_full - mean) / std

    return norm_seed, norm_full, {'day_idx': day_idx, 'start_idx': start}

def generate(model, seed, n_points, ggd_params, noise_scale=1.0):
    """Generate sequence with GGD noise."""
    device = next(model.parameters()).device
    seq_len = len(seed)

    # Convert seed to tensor and initialize generated sequence
    generated = torch.tensor(seed, dtype=torch.float32, device=device)

    with torch.no_grad():
        for _ in range(n_points):
            # Get context window - last seq_len points
            context = generated[-seq_len:].unsqueeze(0)  # Shape: [1, seq_len, 3]
            pred = model(context).squeeze(0)  # Shape: [3]
            #pred = context[0, -1, :]
            # Add GGD noise
            noise = torch.tensor([gennorm.rvs(p['beta'], loc=p['loc'], scale=p['scale'] * noise_scale)
                                 for p in ggd_params], device=device, dtype=torch.float32)
            #noise = 0
            # Append new point
            next_point = (pred + noise).unsqueeze(0)  # Shape: [1, 3]
            generated = torch.cat([generated, next_point], dim=0)

    return generated.cpu().numpy()

def fit_ggd(seed):
    """Fit GGD to increments."""
    inc = seed[1:] - seed[:-1]
    return [dict(zip(['beta', 'loc', 'scale'], gennorm.fit(inc[:, c]))) for c in range(3)]

def plot_time_series(seed, gen, truth, save_dir):
    """Plot time series with ground truth."""
    n_seed = len(seed)
    gen_new = gen[n_seed:]
    truth_new = truth[n_seed:]

    names = ['Bx', 'By', 'Bz']
    colors = ['#e74c3c', '#3498db', '#2ecc71']

    for c in range(3):
        plt.figure(figsize=(12, 4))
        t = np.arange(len(gen))
        plt.plot(t[:n_seed], seed[:, c], 'k-', label='Seed', lw=1.5)
        plt.plot(t[n_seed:], gen_new[:, c], colors[c], label='Generated', lw=1.5)
        plt.plot(t[n_seed:], truth_new[:, c], 'gray', label='Truth', lw=1, ls='--', alpha=0.6)
        plt.axvline(n_seed - 0.5, color='k', ls=':', alpha=0.4)
        plt.title(f'{names[c]} Time Series')
        plt.xlabel('Time from seed start / min')
        plt.ylabel('Component Field / nT')
        plt.xlim(170,400)
        plt.legend()
        plt.grid(alpha=0.3)
        plt.tight_layout()
        if save_dir:
            plt.savefig(f'{save_dir}/timeseries_{names[c].lower()}.png', dpi=150)
        plt.show()

    # Magnitude
    plt.figure(figsize=(12, 4))
    mag_s = np.linalg.norm(seed, axis=1)
    mag_g = np.linalg.norm(gen_new, axis=1)
    mag_t = np.linalg.norm(truth_new, axis=1)
    t = np.arange(len(gen))
    plt.plot(t[:n_seed], mag_s, 'k-', label='Seed', lw=1.5)
    plt.plot(t[n_seed:], mag_g, 'm-', label='Generated', lw=1.5)
    plt.plot(t[n_seed:], mag_t, 'gray', label='Truth', lw=1, ls='--', alpha=0.6)
    plt.axvline(n_seed - 0.5, color='k', ls=':', alpha=0.4)
    plt.title('|B| Magnitude')
    plt.xlabel('Time')
    plt.ylabel('|B|')
    plt.legend()
    plt.grid(alpha=0.3)
    plt.tight_layout()
    if save_dir:
        plt.savefig(f'{save_dir}/timeseries_magnitude.png', dpi=150)
    plt.show()

def plot_spectra(seed, gen, save_dir):
    """Plot power spectral density."""
    n_seed = len(seed)
    gen_new = gen[n_seed:]

    names = ['Bx', 'By', 'Bz']
    colors = ['#e74c3c', '#3498db', '#2ecc71']

    nperseg = min(64, min(n_seed, len(gen_new))//4)

    for c in range(3):
        f_s, psd_s = signal.welch(seed[:, c], nperseg=nperseg)
        f_g, psd_g = signal.welch(gen_new[:, c], nperseg=nperseg)

        plt.figure(figsize=(8, 6))
        plt.loglog(f_s/60, psd_s, 'k-', label='Seed', lw=2)
        plt.loglog(f_g/60, psd_g, colors[c], label='Generated', lw=2)
        plt.title(f'{names[c]} Power Spectrum')
        plt.xlabel('Frequency/Hz')
        plt.ylabel('PSD')
        plt.legend()
        plt.grid(alpha=0.3, which='both')
        plt.tight_layout()

        if save_dir:
            plt.savefig(f'{save_dir}/psd_{names[c].lower()}.png', dpi=150)
        plt.show()

def plot_distributions(seed, gen, ggd_params, save_dir):
    """Plot increment and value distributions."""
    n_seed = len(seed)
    gen_new = gen[n_seed:]

    names = ['Bx', 'By', 'Bz']
    colors = ['#e74c3c', '#3498db', '#2ecc71']

    # Increments with GGD
    for c in range(3):
        seed_inc = seed[1:, c] - seed[:-1, c]
        gen_inc = gen_new[1:, c] - gen_new[:-1, c]

        plt.figure(figsize=(8, 6))
        bins = np.linspace(-3, 3, 100)
        plt.hist(seed_inc, bins, alpha=0.5, density=True, label='Seed', color='gray')
        plt.hist(gen_inc, bins, alpha=0.5, density=True, label='Generated', color=colors[c])

        # GGD fit
        x = np.linspace(-3, 3, 200)
        p = ggd_params[c]
        ggd_pdf = gennorm.pdf(x, p['beta'], loc=p['loc'], scale=p['scale'])
        plt.plot(x, ggd_pdf, 'r--', lw=2, label=f'GGD (β={p["beta"]:.2f})')

        plt.title(f'{names[c]} Increments')
        plt.xlabel('Increment')
        plt.ylabel('Density')
        plt.legend()
        plt.grid(alpha=0.3)
        plt.tight_layout()
        if save_dir:
            plt.savefig(f'{save_dir}/increments_{names[c].lower()}.png', dpi=150)
        plt.show()

    # Structure functions
    plt.figure(figsize=(8, 6))
    lags = [1, 2, 4, 8, 16]

    for order in [2, 4]:
        sf_seed = [np.mean(np.linalg.norm(seed[lag:] - seed[:-lag], axis=1)**order) for lag in lags]
        sf_gen = [np.mean(np.linalg.norm(gen_new[lag:] - gen_new[:-lag], axis=1)**order) for lag in lags]

        plt.loglog(lags, sf_seed, 'o-', label=f'Seed S{order}', ms=8)
        plt.loglog(lags, sf_gen, 's--', label=f'Gen S{order}', ms=8)

    plt.title('Structure Functions')
    plt.xlabel('Lag τ')
    plt.ylabel('S_q(τ)')
    plt.legend()
    plt.grid(alpha=0.3, which='both')
    plt.tight_layout()
    if save_dir:
        plt.savefig(f'{save_dir}/structure_functions.png', dpi=150)
    plt.show()

def run_analysis(model_config, training_config, dataset,
                 n_generate=None, day_idx=None, noise_scale=1.0):
    """Run complete generation analysis."""

    # Setup
    n_generate = n_generate or model_config.seq_len
    save_dir = training_config.checkpoint_dir

    print(f"Generating {n_generate} points from {model_config.seq_len}-point seed")

    # Load model and data
    model = load_model(model_config, save_dir)
    seed, truth, info = get_data(dataset, model_config.seq_len, n_generate, day_idx)

    print(f"Using day {info['day_idx']}, start index {info['start_idx']}")

    # Generate
    ggd_params = fit_ggd(seed)
    generated = generate(model, seed, n_generate, ggd_params, noise_scale)

    # Plot
    plot_time_series(seed, generated, truth, save_dir)
    plot_spectra(seed, generated, save_dir)
    plot_distributions(seed, generated, ggd_params, save_dir)
    plot_autocorrelation(seed, generated, truth, save_dir)  # <-- NEW
    print("Analysis complete!")
    return generated, truth

def _acf_1d(x: np.ndarray, maxlag: int) -> np.ndarray:
    """Biased ACF normalized to 1 at lag 0."""
    x = x.astype(np.float64)
    x = x - x.mean()
    var = x.var() + 1e-12
    n = len(x)
    maxlag = min(maxlag, n - 1)
    acf = np.empty(maxlag + 1, dtype=np.float64)
    for k in range(maxlag + 1):
        acf[k] = np.dot(x[k:], x[:n - k]) / ((n - k) * var)
    return acf

def plot_autocorrelation(seed, gen, truth, save_dir, maxlag=None):
    """Compare autocorrelation of generated vs truth (and seed for reference)."""
    n_seed = len(seed)
    gen_new = gen[n_seed:]
    truth_new = truth[n_seed:]

    names = ['Bx', 'By', 'Bz']
    colors = ['#e74c3c', '#3498db', '#2ecc71']

    # choose a sensible default lag window
    if maxlag is None:
        maxlag = max(1, min(200, (len(gen_new) // 4)))

    for c in range(3):
        acf_seed = _acf_1d(seed[:, c], maxlag)
        acf_gen  = _acf_1d(gen_new[:, c], maxlag)
        acf_true = _acf_1d(truth_new[:, c], maxlag)

        plt.figure(figsize=(8, 6))
        lags = np.arange(maxlag + 1)
        plt.plot(lags, acf_seed, 'k-', label='Seed', lw=1.8, alpha=0.9)
        plt.plot(lags, acf_true, 'gray', label='Truth', lw=1.8, ls='--', alpha=0.9)
        plt.plot(lags, acf_gen,  colors[c], label='Generated', lw=2.2)
        plt.title(f'{names[c]} Autocorrelation')
        plt.xlabel('Lag')
        plt.ylabel('ACF')
        plt.ylim(-1.05, 1.05)
        plt.legend()
        plt.grid(alpha=0.3)
        plt.tight_layout()
        if save_dir:
            plt.savefig(f'{save_dir}/acf_{names[c].lower()}.png', dpi=150)
        plt.show()

    # Also compare |B| magnitude ACF
    mag_seed = np.linalg.norm(seed, axis=1)
    mag_gen  = np.linalg.norm(gen_new, axis=1)
    mag_true = np.linalg.norm(truth_new, axis=1)

    acf_seed_m = _acf_1d(mag_seed, maxlag)
    acf_gen_m  = _acf_1d(mag_gen,  maxlag)
    acf_true_m = _acf_1d(mag_true, maxlag)

    plt.figure(figsize=(8, 6))
    lags = np.arange(maxlag + 1)
    plt.plot(lags, acf_seed_m, 'k-',   label='Seed |B|', lw=1.8, alpha=0.9)
    plt.plot(lags, acf_true_m, 'gray', label='Truth |B|', lw=1.8, ls='--', alpha=0.9)
    plt.plot(lags, acf_gen_m,  'm-',   label='Generated |B|', lw=2.2)
    plt.title('|B| Autocorrelation')
    plt.xlabel('Lag')
    plt.ylabel('ACF')
    plt.ylim(-1.05, 1.05)
    plt.legend()
    plt.grid(alpha=0.3)
    plt.tight_layout()
    if save_dir:
        plt.savefig(f'{save_dir}/acf_magnitude.png', dpi=150)
    plt.show()

# Example usage
if __name__ == "__main__":
    model_config = ModelConfig(
        seq_len=200,
        d_model=64,
        nhead=8,
        num_layers=6,
        dim_feedforward=256,
        dropout=0.1,
        n_features=3
    )

    training_config = TrainingConfig(checkpoint_dir="./checkpoints_3d")

    dataset = TurbulenceDataset(
        npz_path="./daily_B_vectors.npz",
        seq_len=model_config.seq_len,
        max_b_threshold=50.0
    )

    generated, truth = run_analysis(
        model_config, training_config, dataset,
        n_generate=400, day_idx=1, noise_scale=0.7
    )

def plot_average_acf_over_random_seeds(
    model_config,
    training_config,
    dataset,
    n_samples: int = 100,
    n_generate: int | None = None,
    maxlag: int | None = None,
    noise_scale: float = 1.0,
    random_state: int | None = None,
):
    """
    Compute and plot the average ACF across many random seeds and their generated sequences.

    Reuses: load_model, get_data, fit_ggd, generate, _acf_1d

    Args:
        model_config: your ModelConfig (needs .seq_len, etc.)
        training_config: your TrainingConfig (uses .checkpoint_dir for saving)
        dataset: TurbulenceDataset
        n_samples: how many random seeds to average over (default 100)
        n_generate: points to generate beyond the seed (defaults to model_config.seq_len)
        maxlag: ACF max lag (default: min(200, len(gen_new)//4))
        noise_scale: noise scale for GGD noise during generation
        random_state: optional RNG seed for reproducibility

    Saves:
        acf_avg_{bx,by,bz,mag}.png into training_config.checkpoint_dir (if set),
        and also shows the plots.
    """
    if random_state is not None:
        np.random.seed(random_state)

    device_save_dir = getattr(training_config, "checkpoint_dir", None)
    save_dir = device_save_dir

    # Prepare
    n_generate = n_generate or model_config.seq_len
    print(f"[ACF AVG] Sampling {n_samples} seeds | seed_len={model_config.seq_len} | gen_len={n_generate}")

    # Load model once
    model = load_model(model_config, save_dir)

    # We'll determine maxlag from the first sample if not provided
    acf_seed_list = []
    acf_gen_list = []
    acf_truth_list = []
    acf_seed_mag_list = []
    acf_gen_mag_list = []
    acf_truth_mag_list = []

    names = ['Bx', 'By', 'Bz']
    colors = ['#e74c3c', '#3498db', '#2ecc71']

    first_len_gen_new = None
    L = None  # maxlag+1 once determined

    for i in range(n_samples):
        # Random sample (day_idx=None lets get_data choose a valid day/start)
        seed, truth, _ = get_data(dataset, model_config.seq_len, n_generate, day_idx=None)

        # Generate from this seed
        ggd_params = fit_ggd(seed)
        gen = generate(model, seed, n_generate, ggd_params, noise_scale=noise_scale)

        n_seed = len(seed)
        gen_new = gen[n_seed:]
        truth_new = truth[n_seed:]

        if first_len_gen_new is None:
            first_len_gen_new = len(gen_new)
            if maxlag is None:
                maxlag_eff = max(1, min(200, first_len_gen_new // 4))
            else:
                maxlag_eff = maxlag
            L = maxlag_eff + 1

        # Per-component ACFs
        acf_s = np.empty((3, L), dtype=np.float64)
        acf_g = np.empty((3, L), dtype=np.float64)
        acf_t = np.empty((3, L), dtype=np.float64)
        for c in range(3):
            acf_s[c] = _acf_1d(seed[:, c],  maxlag_eff)
            acf_g[c] = _acf_1d(gen_new[:, c], maxlag_eff)
            acf_t[c] = _acf_1d(truth_new[:, c], maxlag_eff)

        acf_seed_list.append(acf_s)
        acf_gen_list.append(acf_g)
        acf_truth_list.append(acf_t)

        # Magnitude ACFs
        mag_seed = np.linalg.norm(seed, axis=1)
        mag_gen  = np.linalg.norm(gen_new, axis=1)
        mag_true = np.linalg.norm(truth_new, axis=1)

        acf_seed_mag_list.append(_acf_1d(mag_seed, maxlag_eff))
        acf_gen_mag_list.append(_acf_1d(mag_gen,  maxlag_eff))
        acf_truth_mag_list.append(_acf_1d(mag_true, maxlag_eff))

        if (i + 1) % max(1, n_samples // 10) == 0:
            print(f"[ACF AVG] Processed {i+1}/{n_samples}")

    # Stack & average
    acf_seed_arr  = np.stack(acf_seed_list, axis=0)   # (N, 3, L)
    acf_gen_arr   = np.stack(acf_gen_list, axis=0)    # (N, 3, L)
    acf_truth_arr = np.stack(acf_truth_list, axis=0)  # (N, 3, L)

    acf_seed_mag_arr  = np.stack(acf_seed_mag_list, axis=0)   # (N, L)
    acf_gen_mag_arr   = np.stack(acf_gen_mag_list, axis=0)    # (N, L)
    acf_truth_mag_arr = np.stack(acf_truth_mag_list, axis=0)  # (N, L)

    acf_seed_mean  = acf_seed_arr.mean(axis=0)   # (3, L)
    acf_gen_mean   = acf_gen_arr.mean(axis=0)    # (3, L)
    acf_truth_mean = acf_truth_arr.mean(axis=0)  # (3, L)

    acf_seed_mag_mean  = acf_seed_mag_arr.mean(axis=0)   # (L,)
    acf_gen_mag_mean   = acf_gen_mag_arr.mean(axis=0)    # (L,)
    acf_truth_mag_mean = acf_truth_mag_arr.mean(axis=0)  # (L,)

    lags = np.arange(L)

    # Plot per component
    for c in range(3):
        plt.figure(figsize=(8, 6))
        plt.plot(lags, acf_seed_mean[c],  'k-', label='Seed (avg)', lw=1.8, alpha=0.9)
        plt.plot(lags, acf_truth_mean[c], 'gray', label='Truth (avg)', lw=1.8, ls='--', alpha=0.9)
        plt.plot(lags, acf_gen_mean[c],   colors[c], label='Generated (avg)', lw=2.2)
        plt.title(f'{names[c]} Average Autocorrelation over {n_samples} samples')
        plt.xlabel('Lag')
        plt.ylabel('ACF')
        plt.ylim(-1.05, 1.05)
        plt.legend()
        plt.grid(alpha=0.3)
        plt.tight_layout()
        if save_dir:
            plt.savefig(f'{save_dir}/acf_avg_{names[c].lower()}.png', dpi=150)
        plt.show()

    # Plot magnitude
    plt.figure(figsize=(8, 6))
    plt.plot(lags, acf_seed_mag_mean,  'k-',   label='Seed |B| (avg)', lw=1.8, alpha=0.9)
    plt.plot(lags, acf_truth_mag_mean, 'gray', label='Truth |B| (avg)', lw=1.8, ls='--', alpha=0.9)
    plt.plot(lags, acf_gen_mag_mean,   'm-',   label='Generated |B| (avg)', lw=2.2)
    plt.title(f'|B| Average Autocorrelation over {n_samples} samples')
    plt.xlabel('Lag')
    plt.ylabel('ACF')
    plt.ylim(-1.05, 1.05)
    plt.legend()
    plt.grid(alpha=0.3)
    plt.tight_layout()
    if save_dir:
        plt.savefig(f'{save_dir}/acf_avg_magnitude.png', dpi=150)
    plt.show()

    return {
        "lags": lags,
        "seed_mean": acf_seed_mean,       # shape (3, L)
        "gen_mean": acf_gen_mean,         # shape (3, L)
        "truth_mean": acf_truth_mean,     # shape (3, L)
        "seed_mag_mean": acf_seed_mag_mean,    # shape (L,)
        "gen_mag_mean": acf_gen_mag_mean,      # shape (L,)
        "truth_mag_mean": acf_truth_mag_mean,  # shape (L,)
    }
# Average ACF over 100 random seeds
_ = plot_average_acf_over_random_seeds(
    model_config,
    training_config,
    dataset,
    n_samples=300,
    n_generate=200,
    noise_scale=1.0,
    random_state=42,   # optional for reproducibility
)



import torch
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm, ks_2samp

def analyze_model_vs_actual_increments(model, dataset, n_sequences=100, seed_len=200):
    """Analyze distribution of model's vs actual first-step increments."""

    device = next(model.parameters()).device
    model_increments = []
    actual_increments = []

    for _ in range(n_sequences):
        # Get random seed sequence with ground truth
        day_idx = np.random.choice(len(dataset.days))
        day = dataset.days[day_idx]

        if len(day) < seed_len + 1:
            continue

        start = np.random.randint(0, len(day) - seed_len - 1)
        raw_seed = day[start:start + seed_len]
        raw_next = day[start + seed_len]  # Actual next point

        # Normalize
        mean, std = raw_seed.mean(axis=0), raw_seed.std(axis=0) + 1e-6
        norm_seed = (raw_seed - mean) / std
        norm_next = (raw_next - mean) / std

        # Get model prediction
        with torch.no_grad():
            context = torch.tensor(norm_seed, dtype=torch.float32, device=device).unsqueeze(0)
            pred = model(context).squeeze(0).cpu().numpy()

        # Calculate increments
        last_point = norm_seed[-1]
        model_inc = pred - last_point
        actual_inc = norm_next - last_point

        model_increments.append(model_inc)
        actual_increments.append(actual_inc)

    return np.array(model_increments), np.array(actual_increments)

def plot_increment_comparison(model_inc, actual_inc, save_path=None):
    """Plot comparison histograms of model vs actual increments."""

    names = ['Bx', 'By', 'Bz']
    colors = ['#e74c3c', '#3498db', '#2ecc71']

    fig, axes = plt.subplots(2, 3, figsize=(15, 10))

    for c, (name, color) in enumerate(zip(names, colors)):
        # Model increments (top row)
        ax_model = axes[0, c]
        m_inc = model_inc[:, c]

        n, bins, _ = ax_model.hist(m_inc, bins=100, density=True, alpha=0.7,
                                   color=color, edgecolor='black', linewidth=0.5)

        mu_m, sigma_m = m_inc.mean(), m_inc.std()
        x = np.linspace(m_inc.min(), m_inc.max(), 100)
        ax_model.plot(x, norm.pdf(x, mu_m, sigma_m), 'k--', lw=2)
        ax_model.axvline(mu_m, color='red', linestyle=':', lw=2, alpha=0.7)
        ax_model.axvline(0, color='black', linestyle='--', lw=1, alpha=0.5)

        ax_model.set_title(f'{name} Model Increments\nμ={mu_m:.4f}, σ={sigma_m:.4f}')
        ax_model.set_ylabel('Density')
        ax_model.grid(alpha=0.3)

        # Actual increments (bottom row)
        ax_actual = axes[1, c]
        a_inc = actual_inc[:, c]

        ax_actual.hist(a_inc, bins=100, density=True, alpha=0.7,
                      color='gray', edgecolor='black', linewidth=0.5)

        mu_a, sigma_a = a_inc.mean(), a_inc.std()
        x = np.linspace(a_inc.min(), a_inc.max(), 100)
        ax_actual.plot(x, norm.pdf(x, mu_a, sigma_a), 'k--', lw=2)
        ax_actual.axvline(mu_a, color='red', linestyle=':', lw=2, alpha=0.7)
        ax_actual.axvline(0, color='black', linestyle='--', lw=1, alpha=0.5)

        ax_actual.set_title(f'{name} Actual Increments\nμ={mu_a:.4f}, σ={sigma_a:.4f}')
        ax_actual.set_xlabel('Increment Value')
        ax_actual.set_ylabel('Density')
        ax_actual.grid(alpha=0.3)

        # KS test
        ks_stat, p_value = ks_2samp(m_inc, a_inc)
        print(f"{name}: KS stat={ks_stat:.4f}, p-value={p_value:.4f}")

    plt.suptitle('Model vs Actual First-Step Increment Distributions', fontsize=14, y=1.02)
    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.show()

    return fig

def plot_overlay_comparison(model_inc, actual_inc, save_path=None):
    """Plot overlaid histograms for direct comparison."""

    names = ['Bx', 'By', 'Bz']
    colors = ['#e74c3c', '#3498db', '#2ecc71']

    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    for c, (ax, name, color) in enumerate(zip(axes, names, colors)):
        m_inc = model_inc[:, c]
        a_inc = actual_inc[:, c]

        # Common bins for both
        all_data = np.concatenate([m_inc, a_inc])
        bins = np.linspace(all_data.min(), all_data.max(), 100)

        # Plot histograms
        ax.hist(a_inc, bins, density=True, alpha=0.5, color='gray',
                edgecolor='black', linewidth=0.5, label='Actual')
        ax.hist(m_inc, bins, density=True, alpha=0.5, color=color,
                edgecolor='black', linewidth=0.5, label='Model')

        # Statistics
        mu_m, sigma_m = m_inc.mean(), m_inc.std()
        mu_a, sigma_a = a_inc.mean(), a_inc.std()

        ax.axvline(mu_m, color=color, linestyle=':', lw=2, alpha=0.7,
                  label=f'Model μ={mu_m:.3f}')
        ax.axvline(mu_a, color='gray', linestyle=':', lw=2, alpha=0.7,
                  label=f'Actual μ={mu_a:.3f}')
        ax.axvline(0, color='black', linestyle='--', lw=1, alpha=0.5)

        ax.set_title(f'{name} Increment Comparison')
        ax.set_xlabel('Increment Value')
        ax.set_ylabel('Density')
        ax.legend(fontsize=9)
        ax.grid(alpha=0.3)

        # Print detailed stats
        print(f"\n{name} Statistics:")
        print(f"  Model:  μ={mu_m:.4f}, σ={sigma_m:.4f}, skew={stats.skew(m_inc):.4f}")
        print(f"  Actual: μ={mu_a:.4f}, σ={sigma_a:.4f}, skew={stats.skew(a_inc):.4f}")
        print(f"  Bias: {mu_m - mu_a:.4f}")
        print(f"  Std ratio: {sigma_m/sigma_a:.4f}")

    plt.suptitle('Model vs Actual Increments (Overlaid)', fontsize=14, y=1.02)
    plt.tight_layout()

    if save_path:
        save_path_overlay = save_path.replace('.png', '_overlay.png')
        plt.savefig(save_path_overlay, dpi=150, bbox_inches='tight')
    plt.show()

    return fig

def run_comparison_analysis(model_config, training_config, dataset, n_sequences=100):
    """Main comparison analysis function."""

    print(f"Analyzing {n_sequences} sequences...")

    # Load model
    model = load_model(model_config, training_config.checkpoint_dir)

    # Collect increments
    model_inc, actual_inc = analyze_model_vs_actual_increments(
        model, dataset, n_sequences, model_config.seq_len
    )

    print(f"\nCollected {len(model_inc)} increment pairs")
    print("\n" + "="*50)
    print("Statistical Comparison:")
    print("="*50)

    # Plot results
    save_path = f"{training_config.checkpoint_dir}/increment_comparison.png" if training_config.checkpoint_dir else None

    # Side-by-side comparison
    plot_increment_comparison(model_inc, actual_inc, save_path)

    # Overlaid comparison
    plot_overlay_comparison(model_inc, actual_inc, save_path)

    # Magnitude analysis
    print("\n" + "="*50)
    print("Magnitude Statistics:")
    print("="*50)
    mag_model = np.linalg.norm(model_inc, axis=1)
    mag_actual = np.linalg.norm(actual_inc, axis=1)
    print(f"Model:  Mean={mag_model.mean():.4f}, Std={mag_model.std():.4f}, Max={mag_model.max():.4f}")
    print(f"Actual: Mean={mag_actual.mean():.4f}, Std={mag_actual.std():.4f}, Max={mag_actual.max():.4f}")
    print(f"Magnitude bias: {mag_model.mean() - mag_actual.mean():.4f}")

    return model_inc, actual_inc

# Usage
if __name__ == "__main__":
    from scipy import stats  # Add this import at the top

    model_config = ModelConfig(
        seq_len=200,
        d_model=64,
        nhead=8,
        num_layers=6,
        dim_feedforward=256,
        dropout=0.1,
        n_features=3
    )

    training_config = TrainingConfig(checkpoint_dir="./checkpoints_3d")

    dataset = TurbulenceDataset(
        npz_path="./daily_B_vectors.npz",
        seq_len=model_config.seq_len,
        max_b_threshold=50.0
    )

    # Run comparison analysis
    model_inc, actual_inc = run_comparison_analysis(
        model_config, training_config, dataset, n_sequences=1000
    )

def generate_seed_gen_pairs(model_config, training_config, dataset,
                             n_pairs=10, n_generate=200, noise_scale=1.0):
    """
    Generates a list of [seed, generated] pairs for reuse in later analysis.

    Returns:
        pairs (list): list of [seed, generated] pairs.
    """
    n_generate = n_generate or model_config.seq_len
    save_dir = getattr(training_config, "checkpoint_dir", None)
    model = load_model(model_config, save_dir)

    pairs = []
    for i in range(n_pairs):
        seed, _, _ = get_data(dataset, model_config.seq_len, n_generate)
        ggd_params = fit_ggd(seed)
        gen = generate(model, seed, n_generate, ggd_params, noise_scale=noise_scale)
        pairs.append([seed, gen])
        if (i + 1) % max(1, n_pairs // 10) == 0:
            print(f"Generated {i+1}/{n_pairs} seed-gen pairs")
    return pairs

# Example usage
seed_gen_pairs = generate_seed_gen_pairs(
    model_config,
    training_config,
    dataset,
    n_pairs=1000,
    n_generate=200,
    noise_scale=1.0
)

with open("my_data.pkl", "wb") as f:
    pickle.dump(seed_gen_pairs, f)

import pickle
import os
from typing import Dict, List, Tuple, Callable, Optional
from dataclasses import dataclass, field
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, random_split
import matplotlib.pyplot as plt
with open("my_data.pkl", "rb") as f:
    seed_gen_pairs = pickle.load(f)

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle
import matplotlib.patches as mpatches

def plot_seed_gen_components(seed_gen_pairs, idx=0, xlim=None, title=None, save_path=None):
    """
    Create publication-quality plot of magnetic field components.

    Args:
        seed_gen_pairs: List of [seed, generated] pairs
        idx: Index of pair to plot
        xlim: Optional x-axis limits (tuple)
        title: Optional custom title
        save_path: Path to save figure
    """
    # Set publication style
    plt.style.use('seaborn-v0_8-whitegrid')
    plt.rcParams.update({
        'font.size': 11,
        'axes.labelsize': 12,
        'axes.titlesize': 13,
        'xtick.labelsize': 10,
        'ytick.labelsize': 10,
        'legend.fontsize': 10,
        'figure.dpi': 100,
        'axes.grid': True,
        'grid.alpha': 0.3,
        'axes.facecolor': '#f8f9fa'
    })

    # Extract data
    pair = seed_gen_pairs[idx]
    seed = pair[0]
    gen = pair[1]
    n_seed = len(seed)

    # Define colors - professional palette
    colors = {
        'x': '#D32F2F',  # Deep red
        'y': '#1976D2',  # Deep blue
        'z': '#388E3C'   # Deep green
    }

    # Create figure with better proportions
    fig, ax = plt.subplots(figsize=(14, 6), facecolor='white')

    # Plot each component
    components = ['x', 'y', 'z']
    for i, comp in enumerate(components):
        # Seed data
        ax.plot(np.arange(n_seed),
               seed[:, i],
               color=colors[comp],
               linewidth=2.0,
               linestyle='--',
               alpha=0.9,
               label=f'$B_{comp}$ (seed)',
               solid_capstyle='round')

        # Generated data
        ax.plot(np.arange(n_seed, len(gen)),
               gen[n_seed:, i],
               color=colors[comp],
               linewidth=2.0,
               alpha=0.8,
               label=f'$B_{comp}$ (generated)',
               solid_capstyle='round')

    # Add subtle shading for seed region
    ax.add_patch(Rectangle((0, ax.get_ylim()[0]),
                           n_seed,
                           ax.get_ylim()[1] - ax.get_ylim()[0],
                           facecolor='gray',
                           alpha=0.05,
                           zorder=0))

    # Boundary line with label
    ax.axvline(n_seed, color='#333333', linestyle=':', linewidth=1.5, alpha=0.7)
    ax.text(n_seed + 2, ax.get_ylim()[1] * 0.9,
           'Generation Start',
           fontsize=9,
           color='#333333',
           rotation=0,
           ha='left')

    # Labels and title
    ax.set_xlabel('Time Step / min', fontweight='medium')
    ax.set_ylabel('Magnetic Field (nT)', fontweight='medium')

    if title is None:
        title = f'Magnetic Field Components: Seed vs Generated'
    ax.set_title(title, fontweight='bold', pad=15)

    # Create custom legend
    seed_patch = mpatches.Patch(color='gray', alpha=0.3, label='Seed Data')
    gen_patch = mpatches.Patch(color='white', alpha=0.8, label='Generated Data')

    # Component lines for legend
    handles = []
    for comp in components:
        handles.append(plt.Line2D([0], [0], color=colors[comp],
                                 linewidth=2, label=f'$B_{comp}$'))

    # Two-column legend
    legend1 = ax.legend(handles=handles,
                       loc='lower left',
                       frameon=True,
                       fancybox=True,
                       shadow=False,
                       ncol=3,
                       columnspacing=1.0,
                       framealpha=0.95,
                       edgecolor='#cccccc')



    # Add the first legend back
    ax.add_artist(legend1)

    # Grid styling
    ax.grid(True, alpha=0.25, linestyle='-', linewidth=0.5)
    ax.set_axisbelow(True)

    # Set limits if specified
    plt.xlim(170,400)

    # Add subtle frame
    for spine in ax.spines.values():
        spine.set_edgecolor('#cccccc')
        spine.set_linewidth(1)

    # Statistics box (optional)
    seed_std = np.std(seed, axis=0)
    gen_std = np.std(gen[n_seed:], axis=0)

    plt.tight_layout()

    plt.show()


# Usage examples:
plot_seed_gen_components(seed_gen_pairs, idx=6, xlim=(0, 400),
                         save_path="./publication_figure.png")

import numpy as np
import matplotlib.pyplot as plt
from scipy import signal

def plot_average_psd(seed_gen_pairs, fs=1.0, max_nperseg=256, save_path=None):
    """
    Compute and plot average PSD using Welch's method with publication-quality styling.
    Creates four separate plot windows: Bx, By, Bz, and |B|.

    Args:
        seed_gen_pairs: List of [seed, gen] pairs
        fs: Sampling frequency (Hz)
        max_nperseg: Upper bound for Welch's nperseg
        save_path: Optional base path to save figures
    """
    # Apply your preferred style
    plt.style.use('seaborn-v0_8-whitegrid')
    plt.rcParams.update({
        'font.size': 11,
        'axes.labelsize': 12,
        'axes.titlesize': 13,
        'xtick.labelsize': 10,
        'ytick.labelsize': 10,
        'legend.fontsize': 10,
        'figure.dpi': 100,
        'axes.grid': True,
        'grid.alpha': 0.3,
        'axes.facecolor': '#f8f9fa'
    })

    # Your color palette
    colors = {
        'x': '#D32F2F',  # Deep red
        'y': '#1976D2',  # Deep blue
        'z': '#388E3C',  # Deep green
        'seed': '#333333',  # Dark gray for seed
        'mag': '#8E24AA'    # Purple for magnitude
    }

    # Calculate nperseg
    min_seed_len = min(len(seed) for seed, *_ in seed_gen_pairs)
    min_gen_len = min(len(gen) - len(seed) for seed, gen, *_ in seed_gen_pairs)
    nperseg = max(8, min(max_nperseg, min(min_seed_len, min_gen_len) // 2))

    # Initialize accumulators
    psd_seed_comp = [None, None, None]
    psd_gen_comp = [None, None, None]
    comp_count = [0, 0, 0]

    psd_seed_mag = None
    psd_gen_mag = None
    mag_count = 0

    freqs = None

    # Process all pairs
    for pair in seed_gen_pairs:
        seed = pair[0]
        gen = pair[1]
        n_seed = len(seed)
        gen_new = gen[n_seed:]  # continuation only

        # Component PSDs
        for c in range(3):
            f_s, Pxx_s = signal.welch(seed[:, c], fs=fs, nperseg=nperseg)
            f_g, Pxx_g = signal.welch(gen_new[:, c], fs=fs, nperseg=nperseg)

            if freqs is None:
                freqs = f_s

            # Accumulate per-component
            if psd_seed_comp[c] is None:
                psd_seed_comp[c] = Pxx_s.copy()
                psd_gen_comp[c] = Pxx_g.copy()
            else:
                psd_seed_comp[c] += Pxx_s
                psd_gen_comp[c] += Pxx_g
            comp_count[c] += 1

        # Magnitude PSD
        mag_seed = np.linalg.norm(seed, axis=1)
        mag_gen = np.linalg.norm(gen_new, axis=1)
        _, Pxx_mag_s = signal.welch(mag_seed, fs=fs, nperseg=nperseg)
        _, Pxx_mag_g = signal.welch(mag_gen, fs=fs, nperseg=nperseg)

        if psd_seed_mag is None:
            psd_seed_mag = Pxx_mag_s.copy()
            psd_gen_mag = Pxx_mag_g.copy()
        else:
            psd_seed_mag += Pxx_mag_s
            psd_gen_mag += Pxx_mag_g
        mag_count += 1

    # Create four separate figures
    comp_names = ['$B_x$', '$B_y$', '$B_z$']
    comp_colors = [colors['x'], colors['y'], colors['z']]

    # Plot 1: Bx
    plt.figure(figsize=(10, 6), facecolor='white')
    plot_component_psd(freqs, psd_seed_comp[0]/comp_count[0],
                      psd_gen_comp[0]/comp_count[0],
                      colors['seed'], comp_colors[0], comp_names[0],
                      fs, nperseg, comp_count[0])
    if save_path:
        plt.savefig(save_path.replace('.png', '_Bx.png'),
                   dpi=300, bbox_inches='tight', facecolor='white')
    plt.show()

    # Plot 2: By
    plt.figure(figsize=(10, 6), facecolor='white')
    plot_component_psd(freqs, psd_seed_comp[1]/comp_count[1],
                      psd_gen_comp[1]/comp_count[1],
                      colors['seed'], comp_colors[1], comp_names[1],
                      fs, nperseg, comp_count[1])
    if save_path:
        plt.savefig(save_path.replace('.png', '_By.png'),
                   dpi=300, bbox_inches='tight', facecolor='white')
    plt.show()

    # Plot 3: Bz
    plt.figure(figsize=(10, 6), facecolor='white')
    plot_component_psd(freqs, psd_seed_comp[2]/comp_count[2],
                      psd_gen_comp[2]/comp_count[2],
                      colors['seed'], comp_colors[2], comp_names[2],
                      fs, nperseg, comp_count[2])
    if save_path:
        plt.savefig(save_path.replace('.png', '_Bz.png'),
                   dpi=300, bbox_inches='tight', facecolor='white')
    plt.show()

    # Plot 4: |B| magnitude
    plt.figure(figsize=(10, 6), facecolor='white')
    plot_component_psd(freqs, psd_seed_mag/mag_count,
                      psd_gen_mag/mag_count,
                      colors['seed'], colors['mag'], '|B|',
                      fs, nperseg, mag_count)
    if save_path:
        plt.savefig(save_path.replace('.png', '_magnitude.png'),
                   dpi=300, bbox_inches='tight', facecolor='white')
    plt.show()

def plot_component_psd(freqs, psd_seed, psd_gen, color_seed, color_gen,
                       component_name, fs, nperseg, count):
    """Plot a single component PSD with consistent styling."""
    ax = plt.gca()

    # Plot lines
    ax.loglog(freqs, psd_seed,
             color=color_seed,
             linewidth=2.5,
             linestyle='--',
             alpha=0.9,
             label=f'Seed {component_name}')

    ax.loglog(freqs, psd_gen,
             color=color_gen,
             linewidth=2.5,
             alpha=0.9,
             label=f'Generated {component_name}')

    # Labels
    ax.set_xlabel('Frequency (Hz)' if fs != 1.0 else 'Frequency (1/sample)',
                  fontweight='medium')
    ax.set_ylabel('Power Spectral Density (nT²/Hz)', fontweight='medium')
    ax.set_title(f'Average PSD — {component_name} Component',
                fontweight='bold', pad=15)

    # Grid
    ax.grid(True, which='both', alpha=0.25, linestyle='-', linewidth=0.5)
    ax.set_axisbelow(True)

    # Frame
    for spine in ax.spines.values():
        spine.set_edgecolor('#cccccc')
        spine.set_linewidth(1)

    # Legend - lower left as per your style
    ax.legend(loc='lower left',
             frameon=True,
             fancybox=True,
             shadow=False,
             framealpha=0.95,
             edgecolor='#cccccc')




    plt.tight_layout()

# Usage
plot_average_psd(seed_gen_pairs, fs=1/60.0, save_path='psd_analysis.png')

import numpy as np
import matplotlib.pyplot as plt
from scipy import signal
from scipy.stats import linregress

def _psd_slope_1d(x, fs=1.0, nperseg=256, fmin=None, fmax=None, overlap=0.5):
    """Calculate PSD slope from linear fit of log10(PSD) vs log10(f)."""
    noverlap = int(nperseg * float(overlap))
    f, Pxx = signal.welch(x, fs=fs, nperseg=nperseg, noverlap=noverlap, detrend='constant')

    # Choose fitting band
    if fmin is None:
        fmin = f[len(f)//10] if len(f) >= 10 else f[1] if len(f) > 1 else f[0]
    if fmax is None:
        fmax = f[-2] if len(f) > 2 else f[-1]

    mask = (f > 0) & (f >= fmin) & (f <= fmax) & np.isfinite(Pxx) & (Pxx > 0)
    f_fit, p_fit = f[mask], Pxx[mask]

    if len(f_fit) < 5:
        return np.nan

    X = np.log10(f_fit)
    y = np.log10(p_fit)
    a, b = np.polyfit(X, y, 1)
    return a

def plot_seed_gen_psd_slope_scatter(
    seed_gen_pairs, fs=1.0, nperseg=None, fmin=None, fmax=None,
    overlap=0.5, title=None, save_path=None
):
    """
    Scatter plot of PSD slopes for seed vs generated data.
    Each component shown in different colors with fitted line.
    """
    # Apply your preferred style
    plt.style.use('seaborn-v0_8-whitegrid')
    plt.rcParams.update({
        'font.size': 11,
        'axes.labelsize': 12,
        'axes.titlesize': 13,
        'xtick.labelsize': 10,
        'ytick.labelsize': 10,
        'legend.fontsize': 10,
        'figure.dpi': 100,
        'axes.grid': True,
        'grid.alpha': 0.3,
        'axes.facecolor': '#f8f9fa'
    })

    # Your color palette
    colors = ['#D32F2F', '#1976D2', '#388E3C']  # Red, Blue, Green
    comp_names = ['$B_x$', '$B_y$', '$B_z$']

    # Determine nperseg
    min_seed_len = min(len(seed) for seed, *_ in seed_gen_pairs)
    min_gen_len = min(len(gen) - len(seed) for seed, gen, *_ in seed_gen_pairs)
    if nperseg is None:
        nperseg = max(32, min(min_seed_len, min_gen_len) // 2)

    # Collect slopes by component
    slopes_seed = [[], [], []]  # For Bx, By, Bz
    slopes_gen = [[], [], []]

    # Process all pairs
    for seed, gen, *_ in seed_gen_pairs[:200]:
        n_seed = len(seed)
        cont = gen[n_seed:]  # continuation only

        if len(seed) < nperseg or len(cont) < nperseg:
            continue

        for c in range(3):
            s_seed = _psd_slope_1d(seed[:, c], fs=fs, nperseg=nperseg,
                                   fmin=fmin, fmax=fmax, overlap=overlap)
            s_gen = _psd_slope_1d(cont[:, c], fs=fs, nperseg=nperseg,
                                 fmin=fmin, fmax=fmax, overlap=overlap)

            if np.isfinite(s_seed) and np.isfinite(s_gen):
                slopes_seed[c].append(s_seed)
                slopes_gen[c].append(s_gen)

    # Check if we have data
    total_points = sum(len(slopes_seed[c]) for c in range(3))
    if total_points == 0:
        raise ValueError("No valid pairs to compute slopes. Check nperseg/lengths.")

    # Calculate bounds for square plot
    all_slopes = []
    for c in range(3):
        all_slopes.extend(slopes_seed[c])
        all_slopes.extend(slopes_gen[c])

    xy_min = np.min(all_slopes)
    xy_max = np.max(all_slopes)
    pad = 0.1 * (xy_max - xy_min if xy_max > xy_min else 1.0)
    lo, hi = xy_min - pad, xy_max + pad

    # Create figure
    fig, ax = plt.subplots(figsize=(8, 8), facecolor='white')

    # Collect all points for overall fit
    all_seed_slopes = []
    all_gen_slopes = []

    # Plot each component with different colors
    for c in range(3):
        if len(slopes_seed[c]) > 0:
            ax.scatter(slopes_seed[c], slopes_gen[c],
                      s=60,
                      alpha=0.7,
                      color=colors[c],
                      edgecolors='white',
                      linewidth=0.5,
                      label=f'{comp_names[c]} ')

            # Collect for overall fit
            all_seed_slopes.extend(slopes_seed[c])
            all_gen_slopes.extend(slopes_gen[c])

    # Fit line to all data
    if len(all_seed_slopes) > 1:
        result = linregress(all_seed_slopes, all_gen_slopes)
        slope, intercept, r_value = result.slope, result.intercept, result.rvalue

        # Plot fitted line
        x_fit = np.array([lo, hi])
        y_fit = slope * x_fit + intercept
        ax.plot(x_fit, y_fit,
               color='#8E24AA',  # Purple for fit
               linewidth=2.5,
               alpha=0.9,
               label=f'Fit: y={slope:.2f}x{intercept:+.2f} (r={r_value:.3f})')

    # Perfect agreement line (y=x)
    ax.plot([lo, hi], [lo, hi],
           color='#333333',
           linestyle='--',
           linewidth=2,
           alpha=0.8,
           label='y = x',
           zorder=0)

    # Add ±10% error bounds (optional visual guide)
    ax.fill_between([lo, hi],
                   [0.9*lo, 0.9*hi],
                   [1.1*lo, 1.1*hi],
                   color='gray',
                   alpha=0.05,
                   zorder=0)

    # Labels and title
    ax.set_xlabel('Seed PSD Slope', fontweight='medium')
    ax.set_ylabel('Generated PSD Slope', fontweight='medium')
    ax.set_title(title or 'PSD Spectral Slope Comparison', fontweight='bold', pad=15)

    # Square aspect ratio
    ax.set_xlim(lo, hi)
    ax.set_ylim(lo, hi)
    ax.set_aspect('equal', 'box')

    # Grid
    ax.grid(True, alpha=0.25, linestyle='-', linewidth=0.5)
    ax.set_axisbelow(True)

    # Frame
    for spine in ax.spines.values():
        spine.set_edgecolor('#cccccc')
        spine.set_linewidth(1)

    # Legend
    ax.legend(loc='upper left',
             frameon=True,
             fancybox=True,
             shadow=False,
             framealpha=0.95,
             edgecolor='#cccccc')

    # Add frequency range info
    freq_text = f'nperseg = {nperseg}'
    if fmin is not None or fmax is not None:
        freq_text += f'\nFreq range: [{fmin or "auto":.2e}, {fmax or "auto":.2e}] Hz'

    ax.text(0.02, 0.98, freq_text,
           transform=ax.transAxes,
           fontsize=8,
           verticalalignment='top',
           bbox=dict(boxstyle='round,pad=0.3',
                    facecolor='white',
                    edgecolor='#cccccc',
                    alpha=0.9))

    plt.tight_layout()

    plt.show()

# Usage:
plot_seed_gen_psd_slope_scatter(seed_gen_pairs, fs=1/60.0)

from scipy import signal
from scipy.stats import gennorm
import numpy as np
import matplotlib.pyplot as plt

def _acf_1d(x, maxlag):
    x = np.asarray(x, dtype=np.float64)
    x -= x.mean()
    var = x.var() + 1e-12
    n = len(x)
    maxlag = min(maxlag, n - 1)
    acf = np.empty(maxlag + 1)
    for k in range(maxlag + 1):
        acf[k] = np.dot(x[k:], x[:n - k]) / ((n - k) * var)
    return acf
# --- helpers (unchanged logic, trimmed) ---
def _psd_amp(seed_1d, L):
    f, psd = signal.welch(seed_1d, nperseg=min(128, max(8, len(seed_1d)//4)))
    psd = np.maximum(psd, 1e-12)
    freqs = np.fft.rfftfreq(L, d=1.0)
    return np.sqrt(np.interp(freqs, f, psd, left=psd[0], right=psd[-1]))

def _surrogate_spec_interm(seed_1d, L, iters=2):
    """IAAFT-like: enforce spectrum + match increment GGD, repeated."""
    amp = _psd_amp(seed_1d, L)
    beta, loc, scale = gennorm.fit(np.diff(seed_1d))
    phase = np.exp(1j * 2 * np.pi * np.random.rand(len(amp)))
    phase[0] = 1.0 + 0j
    if (len(amp) > 1) and ((len(amp)-1)*2 == L): phase[-1] = 1.0 + 0j
    y = np.fft.irfft(amp * phase, n=L)
    for _ in range(max(1, iters)):
        Y = np.fft.rfft(y)
        y = np.fft.irfft(amp * np.exp(1j*np.angle(Y)), n=L)
        inc = np.diff(y, prepend=y[0])
        tgt = gennorm.rvs(beta, loc=loc, scale=scale, size=len(inc))
        inc_map = np.empty_like(inc); inc_map[np.argsort(inc)] = np.sort(tgt)
        y = np.cumsum(inc_map)
    return y

def _acf_mse(seed, test, maxlag):
    return (_acf_1d(test, maxlag) - _acf_1d(seed, maxlag))**2

# --- main function: uses precomputed pairs ---
def plot_acf_mse_vs_baseline_from_pairs(
    seed_gen_pairs,
    maxlag=None,
    save_dir=None,
    iters_surrogate=2,
    xlim=(0, 50),
    logy=True
):
    """
    seed_gen_pairs: list of [seed, gen] where gen = [seed ; continuation], shapes (T, 3)
    Plots avg per-lag ACF MSE for Generated vs Seed and Surrogate vs Seed,
    for Bx, By, Bz, and |B|.
    """
    names  = ['Bx', 'By', 'Bz']
    colors = ['#e74c3c', '#3498db', '#2ecc71']

    # pick a single Lmax from first valid pair
    Lmax = None
    for seed, gen, *rest in seed_gen_pairs:
        n_seed = len(seed)
        cont_len = len(gen) - n_seed
        if cont_len > 4:  # anything > few points
            Lmax = max(1, min(200, cont_len // 4)) if maxlag is None else maxlag
            break
    if Lmax is None:
        raise ValueError("No valid pairs with continuation long enough to compute ACF.")

    # components + |B|
    for c, name in enumerate(names + ['|B|']):
        se_gen_all, se_sur_all = [], []
        for seed, gen, *rest in seed_gen_pairs:
            n_seed = len(seed)
            cont = gen[n_seed:]
            if len(cont) <= Lmax or len(seed) <= Lmax:
                continue
            if name != '|B|':
                s1, g1 = seed[:, c], cont[:, c]
            else:
                s1, g1 = np.linalg.norm(seed, axis=1), np.linalg.norm(cont, axis=1)

            sur = _surrogate_spec_interm(s1, len(g1), iters=iters_surrogate)
            se_gen_all.append(_acf_mse(s1, g1, Lmax))
            se_sur_all.append(_acf_mse(s1, sur, Lmax))

        if not se_gen_all:
            print(f"[WARN] No valid data for {name}; skipping.")
            continue

        se_gen_mean = np.mean(se_gen_all, axis=0)
        se_sur_mean = np.mean(se_sur_all, axis=0)
        lags = np.arange(Lmax + 1)

        plt.figure(figsize=(8,5))
        col = colors[c] if name != '|B|' else 'm'
        plt.plot(lags, se_gen_mean, col, lw=2, label=f'Generated (mean MSE={se_gen_mean.mean():.2e})')
        plt.plot(lags, se_sur_mean, 'k--', lw=2, alpha=0.85, label=f'Surrogate (mean MSE={se_sur_mean.mean():.2e})')

        if xlim is not None: plt.xlim(*xlim)
        plt.grid(alpha=0.3)
        plt.xlabel('Lag'); plt.ylabel('ACF Squared Error')
        plt.ylim(0.01,1)
        plt.title(f'ACF Error vs. Seed — {name}')
        plt.legend(); plt.tight_layout()
        if save_dir:
            plt.savefig(f'{save_dir}/acf_mse_pairs_{name.replace("|","mag").lower()}.png', dpi=150)
        plt.show()

# --- example usage ---
plot_acf_mse_vs_baseline_from_pairs(seed_gen_pairs, maxlag=100, save_dir="./checkpoints_3d")

import numpy as np
import matplotlib.pyplot as plt

def plot_average_structure_functions_from_pairs(
    seed_gen_pairs,
    lags=(1, 2, 4, 8, 16, 32, 64),
    orders=(2, 4),
    save_dir: str | None = None,
    style=True,
):
    """
    Compute and plot average vector structure functions from precomputed pairs.
    Each pair is [seed, gen] or [seed, gen, truth], where gen = [seed ; continuation].
    Uses only the generated continuation (post-seed) for 'gen'.
    """
    if style:
        plt.style.use('seaborn-v0_8-whitegrid')
        plt.rcParams.update({
            'font.size': 11, 'axes.labelsize': 12, 'axes.titlesize': 13,
            'xtick.labelsize': 10, 'ytick.labelsize': 10, 'legend.fontsize': 10,
            'figure.dpi': 100, 'axes.grid': True, 'grid.alpha': 0.3,
            'axes.facecolor': '#f8f9fa'
        })

    lags = np.asarray(lags, dtype=int)
    Q, T = len(orders), len(lags)

    # Accumulators (one row per pair)
    sf_seed_all = []
    sf_gen_all  = []

    def vec_inc(X, lag):
        d = X[lag:] - X[:-lag]      # shape (N-lag, 3)
        return np.linalg.norm(d, axis=1)

    # Collect structure functions for each pair
    for pair in seed_gen_pairs:
        seed, gen = pair[0], pair[1]
        n_seed = len(seed)
        gen_new = gen[n_seed:]      # continuation only

        # Skip pairs that are too short for requested lags
        if len(seed) <= lags.min() or len(gen_new) <= lags.min():
            continue

        sf_seed = np.full((Q, T), np.nan, dtype=float)
        sf_gen  = np.full((Q, T), np.nan, dtype=float)

        for oi, q in enumerate(orders):
            for ti, lag in enumerate(lags):
                if lag < len(seed):
                    sf_seed[oi, ti] = np.mean(vec_inc(seed, lag)**q)
                if lag < len(gen_new):
                    sf_gen[oi, ti]  = np.mean(vec_inc(gen_new, lag)**q)

        sf_seed_all.append(sf_seed)
        sf_gen_all.append(sf_gen)

    if len(sf_gen_all) == 0:
        raise ValueError("No valid pairs after lag filtering. Use smaller lags or longer continuations.")

    sf_seed_all = np.stack(sf_seed_all, axis=0)  # (N, Q, T)
    sf_gen_all  = np.stack(sf_gen_all,  axis=0)  # (N, Q, T)

    # Statistics (mean & std across pairs), ignoring NaNs from oversize lags
    sf_seed_mean = np.nanmean(sf_seed_all, axis=0)
    sf_gen_mean  = np.nanmean(sf_gen_all,  axis=0)
    sf_seed_std  = np.nanstd(sf_seed_all,  axis=0)
    sf_gen_std   = np.nanstd(sf_gen_all,   axis=0)

    # Colors
    colors = {
        'seed': '#333333',  # Dark gray
        'gen2': '#D32F2F',  # Deep red for q=2
        'gen4': '#1976D2',  # Deep blue for q=4
    }

    # Single figure for both orders
    fig, ax = plt.subplots(figsize=(10, 6), facecolor='white')

    for oi, q in enumerate(orders):
        m = np.isfinite(sf_seed_mean[oi]) & np.isfinite(sf_gen_mean[oi])
        if not np.any(m):
            continue
        gen_color = colors['gen2'] if q == 2 else colors['gen4']
        mark = 'o' if q == 2 else 's'

        # Seed
        ax.loglog(lags[m], sf_seed_mean[oi, m],
                  color=colors['seed'], linestyle='--', linewidth=2.5,
                  marker=mark, markersize=7, markerfacecolor='white',
                  markeredgecolor=colors['seed'], markeredgewidth=2,
                  alpha=0.9, label=f'Seed $S_{{{q}}}$')

        # Generated
        ax.loglog(lags[m], sf_gen_mean[oi, m],
                  color=gen_color, linestyle='-', linewidth=2.5,
                  marker='o', markersize=7, markerfacecolor=gen_color,
                  markeredgecolor='white', markeredgewidth=1,
                  alpha=0.9, label=f'Generated $S_{{{q}}}$')

    ax.set_xlabel('Lag $\\tau$ / min', fontweight='medium')
    ax.set_ylabel('Structure Function $S_q(\\tau)$ / $nT^n$', fontweight='medium')
    ax.set_title(f'Structure Function Comparison',
                 fontweight='medium', pad=15)
    ax.grid(True, which='both', alpha=0.25, linestyle='-', linewidth=0.5)
    ax.set_axisbelow(True)
    for spine in ax.spines.values():
        spine.set_edgecolor('#cccccc'); spine.set_linewidth(1)
    ax.legend(loc='lower right', frameon=True, fancybox=True, framealpha=0.95,
              edgecolor='#cccccc', ncol=2)

    plt.tight_layout()
    if save_dir:
        plt.savefig(f'{save_dir}/structure_functions_from_pairs.png',
                    dpi=300, bbox_inches='tight', facecolor='white')
    plt.show()

    return {
        "lags": lags,
        "orders": orders,
        "seed_mean": sf_seed_mean,  # (Q, T)
        "gen_mean":  sf_gen_mean,   # (Q, T)
        "seed_std":  sf_seed_std,   # (Q, T)
        "gen_std":   sf_gen_std,    # (Q, T)
        "num_pairs_used": sf_gen_all.shape[0],
    }
# seed_gen_pairs = [[seed, gen], ...]  # already built
res = plot_average_structure_functions_from_pairs(
    seed_gen_pairs,
    lags=(1,2,4,8,16,32,64),
    orders=(2,4),
    save_dir="./checkpoints_3d"
)

import numpy as np
import matplotlib.pyplot as plt

def plot_mean_centered_generated_components(seed_gen_pairs):
    """
    For each [seed, gen] pair:
      1. Compute the mean of each component of the seed sequence
      2. Subtract these means from the generated sequence (component-wise)
    Then average each component over all pairs and plot separately.

    Args:
        seed_gen_pairs: list of [seed, gen] arrays
                        seed.shape = (T_seed, 3)
                        gen.shape  = (T_gen, 3)
    """
    n_pairs = len(seed_gen_pairs)
    T_gen = seed_gen_pairs[0][1].shape[0]
    n_comp = seed_gen_pairs[0][1].shape[1]

    all_centered = np.zeros((n_pairs, T_gen, n_comp))

    for i, (seed, gen) in enumerate(seed_gen_pairs):
        seed_mean = seed.mean(axis=0)            # mean per component
        gen_centered = gen - seed_mean           # subtract seed mean per component
        all_centered[i] = abs(gen_centered)

    avg_centered = all_centered.mean(axis=0)     # (T_gen, 3)

    # Plot per component
    comp_names = ['Bx', 'By', 'Bz']
    colors = ['#e74c3c', '#3498db', '#2ecc71']

    plt.figure(figsize=(9, 5))
    for c in range(n_comp):
        plt.plot(avg_centered[:, c], lw=2, color=colors[c], label=comp_names[c])

    plt.xlabel('Time step')
    plt.ylabel('Average mean-centered value')
    plt.title('Average mean-centered generated components')
    plt.xlim(100,400)
    plt.grid(alpha=0.3)
    plt.legend()
    plt.tight_layout()
    plt.show()

    return avg_centered

# Example:
avg_centered = plot_mean_centered_generated_components(seed_gen_pairs)

import numpy as np
import matplotlib.pyplot as plt

def plot_average_acf_vertical(seed_gen_pairs, maxlag=None, save_path=None):
    """
    Plot Bx, By, Bz, and |B| ACF means as a vertical stack of subplots
    sharing the same x-axis, with labels moved into legends.
    """
    names  = ['Bx', 'By', 'Bz', '|B|']
    colors = ['#e74c3c', '#3498db', '#2ecc71', 'm']

    # Determine effective maxlag
    L = None
    maxlag_eff = None
    for seed, gen, *rest in seed_gen_pairs:
        cont_len = len(gen) - len(seed)
        if cont_len > 4:
            maxlag_eff = max(1, min(200, cont_len // 4)) if maxlag is None else maxlag
            L = maxlag_eff + 1
            break
    if L is None:
        raise ValueError("No valid pairs with continuation long enough.")

    def _acf_1d(x, maxlag):
        x = np.asarray(x, dtype=np.float64)
        x -= x.mean()
        var = x.var() + 1e-12
        n = len(x)
        maxlag = min(maxlag, n - 1)
        acf = np.empty(maxlag + 1)
        for k in range(maxlag + 1):
            acf[k] = np.dot(x[k:], x[:n - k]) / ((n - k) * var)
        return acf

    # Storage
    acf_seed = []
    acf_gen  = []
    acf_mag_seed = []
    acf_mag_gen  = []

    for seed, gen, *rest in seed_gen_pairs:
        n_seed = len(seed)
        gen_new = gen[n_seed:]
        if len(gen_new) <= maxlag_eff or len(seed) <= maxlag_eff:
            continue

        acf_seed.append([_acf_1d(seed[:, c], maxlag_eff) for c in range(3)])
        acf_gen.append([_acf_1d(gen_new[:, c], maxlag_eff) for c in range(3)])

        mag_seed = np.linalg.norm(seed, axis=1)
        mag_gen  = np.linalg.norm(gen_new, axis=1)
        acf_mag_seed.append(_acf_1d(mag_seed, maxlag_eff))
        acf_mag_gen.append(_acf_1d(mag_gen,  maxlag_eff))

    acf_seed = np.mean(acf_seed, axis=0)         # (3, L)
    acf_gen  = np.mean(acf_gen, axis=0)          # (3, L)
    acf_mag_seed = np.mean(acf_mag_seed, axis=0) # (L,)
    acf_mag_gen  = np.mean(acf_mag_gen, axis=0)  # (L,)

    lags = np.arange(L)

    fig, axes = plt.subplots(4, 1, figsize=(8, 10), sharex=True)

    for i in range(3):
        axes[i].plot(lags, acf_seed[i], 'k-', label=f'{names[i]} Seed', lw=1.8, alpha=0.9)
        axes[i].plot(lags, acf_gen[i], colors[i], label=f'{names[i]} Generated', lw=2.0)
        axes[i].set_ylabel('ACF')
        axes[i].set_ylim(-0.4, 1.05)
        axes[i].grid(alpha=0.3)
        axes[i].legend(loc='upper right', fontsize=9, frameon=False)

    # Magnitude subplot
    axes[3].plot(lags, acf_mag_seed, 'k-', label='|B| Seed', lw=1.8, alpha=0.9)
    axes[3].plot(lags, acf_mag_gen, colors[3], label='|B| Generated', lw=2.0)
    axes[3].set_ylabel('ACF')
    axes[3].set_xlabel('Lag / min')
    axes[3].set_ylim(-0.4, 1.05)
    axes[3].grid(alpha=0.3)
    axes[3].legend(loc='upper right', fontsize=9, frameon=False)

    plt.tight_layout(h_pad=0.2)

    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.show()
plot_average_acf_vertical(seed_gen_pairs, maxlag=100, save_path="./checkpoints_3d/acf_vertical.png")